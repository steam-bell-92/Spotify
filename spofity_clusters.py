# -*- coding: utf-8 -*-
"""Spofity_Clusters.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FD047jOo-57-PLplNl-WGFFo2KZsHVed

---
# ***PROBLEM STATEMENT***
---

This project analyzes over 1.2 million Spotify tracks by clustering them based on audio features using K-Means. After cleaning the dataset—removing missing values, unnecessary columns & details—the Elbow Method is used to determine the optimal number of clusters. Each cluster is then named based on representative track characteristics, and the distribution of songs across these custom-named clusters is explored.

---

## Exploratory Data Analysis (EDA)
---

Importing all required libraries.
"""

# Commented out IPython magic to ensure Python compatibility.
# For Data analysis
import numpy as np
import pandas as pd
# For Data Visualization
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import plotly.express as px
# For training & testing model
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans, MiniBatchKMeans
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, accuracy_score

"""Reading the dataset by mounting google drive."""

from google.colab import drive        # mounting drive as it would be faster to load an large dataset (300mb+)
drive.mount('/content/drive')

path = "/content/drive/MyDrive/tracks_features.csv"
df = pd.read_csv(path)

df.head()

"""Naming the columns explicitly."""

cols = ['id', 'album', 'album_id', 'artist_ids', 'track_number', 'release_date', 'disc_number', 'time_signature', 'key', 'mode', 'year']
df.drop(cols, axis=1, inplace=True)
df.head()

"""Cleaned the `artists` column by removing punctuation to improve readability."""

df['artists'].copy().replace(r'\[|\]|\'', '', regex=True, inplace=True)
df.head()

"""Obtaining shape of dataset."""

df.shape

"""Removed rows with missing (NaN) values from the dataset using dropna()."""

df.dropna(inplace=True)
df.shape

"""Summary of dataset."""

df.describe()

"""Obtaining the datatype of dataset."""

df.info()

"""### Intoducing a new parameter: `time` & dropping `duration_ms` column"""

df['time'] = df['duration_ms'] / 60000        # Better to deal in minutes than milli-seconds    (1 min = 60000 ms)

df.drop('duration_ms', inplace=True, axis=1)
df.head()

"""Splitting the dataset between training and test set equally."""

train_df, test_df = train_test_split(df, test_size=0.5, random_state=42)

"""### Introducing a new parameter: `explicit_label`"""

train_df['explicit_label'] = train_df['explicit'].map({False: 'Non-Explicit', True: 'Explicit'})

"""---
## Data Visualization
---

---
### Plotting a `KDE Plot` for distribution of `danceability` for explicit & non-explicit songs
---
"""

sns.kdeplot(x='danceability', data=train_df, thresh=0.05, fill=True, hue='explicit_label')
plt.xlabel('Danceability')
plt.ylabel('Probability Density')
plt.title('Distribution of Danceability')
plt.show()

"""---
Observation:

- Both explicit and non-explicit songs follow a similar distribution.
- Explicit songs have a slightly higher density in mid-to-high danceability (0.6–0.8).
- Suggests that explicit songs tend to be more danceable on average.
---

---
### Plotting a `KDE Plot` for distribution of `energy` for explicit & non-explicit songs
---
"""

sns.kdeplot(x='energy', data=train_df, thresh=0.05, fill=True, hue='explicit_label')
plt.xlabel('Energy')
plt.ylabel('Probability Density')
plt.title('Distribution of Energy')
plt.show()

fig = px.density_contour(train_df, x='energy', y='danceability', color='explicit_label', title='Energy vs Danceability')
fig.update_layout(xaxis_title='Energy', yaxis_title='Danceability')
fig.show()

"""---
Observation:
- Energy values for explicit songs are more uniformly distributed.
- Non-explicit songs show peaks at lower energy levels, suggesting they are more mellow.
- This could imply explicit songs tend to be more energetic.
---

### Dropped the `explicit_label` column as it was no longer needed for model training & testing.
"""

train_df.drop('explicit_label', axis=1, inplace=True)

"""---
### Plotting a Correlation Matrix
---
"""

plt.figure(figsize=(10,8))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""---
Observation:
- Loud songs are usually high in energy
- Acoustic songs tend to be soft and calm
- Danceable songs are often happy
- Instrumental tracks have little or no speech
---

---
## Machine Learning Workflow
---

### Scaling the required columns by `MinMaxScaler()`.
"""

scaler = MinMaxScaler()

column = ['artists', 'name']
train = train_df.copy().drop(column, axis=1)
test = test_df.copy().drop(column, axis=1)

train_df_scaled = scaler.fit_transform(train)
test_df_scaled = scaler.transform(test)

"""### Trained `K-Means` with varying cluster counts and applied the `Elbow Method` to select the best one."""

points = []

for k in range(2, 11) :
  kmeans = KMeans(n_clusters=k, random_state=42)
  kmeans.fit(train_df_scaled)
  y_pred = kmeans.predict(train_df_scaled)
  points.append(kmeans.inertia_)
  SS = silhouette_score(train_df_scaled[:10000], y_pred[:10000])
  print(SS)
fig1 = px.line(x=range(2, 11), y=points, title='Elbow Method')
fig1.show()

"""Selected the best model from the previously trained epochs."""

best = KMeans(n_clusters=6, random_state=42)
best.fit(train_df_scaled)
test['clusters'] = best.predict(test_df_scaled)

"""Calculating Calinski-Karabasz Score"""

CHS = calinski_harabasz_score(train_df_scaled, best.labels_)
print("Calinski-Harabasz Score:" ,CHS)

"""Calculating Davies-Bouldin Score"""

DBS = davies_bouldin_score(train_df_scaled, best.labels_)
print("Davies-Bouldin Score: " ,DBS)

"""Calculating Silhouette Score"""

SS = silhouette_score(train_df_scaled[:10000], best.labels_[:10000])
print("Silhouette Score:" ,SS)

"""### Retrieved sample data points from each cluster."""

for i in range(test['clusters'].nunique()):
    print(f"\nCluster {i}:")
    print(test[test['clusters'] == i].head(3))

"""### Named the clusters based on key values observed earlier."""

cluster_tags = {                                            # Tags are defined as per my intutiuon on song features
    0: "Acoustic Pop",
    1: "Workout Instrumentals",
    2: "Danceable Pop",
    3: "Ambient Electronic",
    4: "Indie Acoustic Vocal Tracks",
    5: "Trap Beats"
}

test['cluster_tag'] = test['clusters'].map(cluster_tags)

test.drop('clusters', axis=1, inplace=True)

"""Calculated the number of data points in each custom-labeled cluster."""

test['cluster_tag'].value_counts()